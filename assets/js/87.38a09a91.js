(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{663:function(t,e,r){"use strict";r.r(e);var n=r(7),s=Object(n.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h2",{attrs:{id:"问题记录"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#问题记录"}},[t._v("#")]),t._v(" 问题记录")]),t._v(" "),r("p",[t._v("Q1：K折交叉验证的过程明白，但是如何达到减小过拟合的问题呢？ A1：如果不使用交叉验证： 1.\n最终模型与参数的选取将极大程度依赖于你对训练集和测试集的划分方法 2. 只用了部分数据进行模型的训练 3. 因此使用交叉验证，可以提高了模型的泛化能力")]),t._v(" "),r("h2",{attrs:{id:"模型融合目标"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#模型融合目标"}},[t._v("#")]),t._v(" 模型融合目标")]),t._v(" "),r("p",[t._v("对于多种调参完成的模型进行模型融合")]),t._v(" "),r("h2",{attrs:{id:"内容介绍"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#内容介绍"}},[t._v("#")]),t._v(" 内容介绍")]),t._v(" "),r("ol",[r("li",[t._v("简单加权融合：")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 回归：算术平均融合（n个数据叠加除以n），集合平均融合（n个数据相乘后开n次方）\n* 分类：投票\n* 综合：排序融合，log融合\n* 补充：回归也叫分类概率，分类模型的预测值为结果为某一分类结果的概率\n")])])]),r("ol",{attrs:{start:"2"}},[r("li",[t._v("stacking/blending（堆放/混合）：")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 构建多层模型，次级模型利用初级模型的预测结果再拟合预测\n")])])]),r("ol",{attrs:{start:"3"}},[r("li",[t._v("boosting/bagging（在xgboost，Adaboost，GBDT中已经用到）")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 多树的提升方法\n")])])]),r("h2",{attrs:{id:"stacking相关理论"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#stacking相关理论"}},[t._v("#")]),t._v(" Stacking相关理论")]),t._v(" "),r("ol",[r("li",[t._v("概念介绍")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 简单来说 stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。\n* 在stacking方法中，我们把个体学习器叫做初级学习器，用于结合的学习器叫做次级学习器或元学习器（meta-learner），次级学习器用于训练的数据叫做次级训练集。次级训练集是在训练集上用初级学习器得到的。\n")])])]),r("ol",{attrs:{start:"2"}},[r("li",[t._v("方法讲解")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 对于二级Stacking分析\n* 以train集和真实的标签列训练，得到个体模型，用此模型对train和test集进行预测，得到标签列P1，T1\n* 同理得到P2,T2\n* 将P1，P2，T1，T2分别合并，得到新的训练集train2和新的测试集test2\n* 以train2为特征，train中的真实标签训练，得到次级模型，将test2输入模型，得到最终的标签列Ypre\n")])])]),r("ol",{attrs:{start:"3"}},[r("li",[t._v("问题讲解")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 接这样有时对于如果训练集和测试集分布不那么一致的情况下是有一点问题\n* 在于用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集\n* 模型在测试集上的泛化能力或者说效果会有一定的下降\n")])])]),r("ol",{attrs:{start:"4"}},[r("li",[t._v("问题解决")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 次级模型尽量选择简单的线性模型\n* 利用K折交叉验证\n")])])]),r("h2",{attrs:{id:"代码示例"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#代码示例"}},[t._v("#")]),t._v(" 代码示例")]),t._v(" "),r("h3",{attrs:{id:"回归融合"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#回归融合"}},[t._v("#")]),t._v(" 回归融合")]),t._v(" "),r("ol",[r("li",[t._v("简单加权平均，结果直接融合")])]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值\ntest_pre1 = [1.2, 3.2, 2.1, 6.2]\ntest_pre2 = [0.9, 3.1, 2.0, 5.9]\ntest_pre3 = [1.1, 2.9, 2.2, 6.0]")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("# y_test_true 代表第模型的真实值\ny_test_true = [1, 3, 2, 6] \n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("# 定义结果的加权平均函数\ndef Weighted_method(test_pre1, test_pre2, test_pre3, w=[1/3, 1/3, 1/3]):\n    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n    return Weighted_result\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\nfrom sklearn import metrics\nprint('Pred1 MAE:', metrics.mean_absolute_error(y_test_true, test_pre1))\nprint('Pred2 MAE:', metrics.mean_absolute_error(y_test_true, test_pre2))\nprint('Pred3 MAE:', metrics.mean_absolute_error(y_test_true, test_pre3))")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\nPred1 MAE: 0.1750000000000001\nPred2 MAE: 0.07499999999999993\nPred3 MAE: 0.10000000000000009")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n# 根据加权计算MAE\nw = [0.3, 0.4, 0.3]\nWeighted_pre = Weighted_method(test_pre1, test_pre2, test_pre3, w)\nprint('Weighted_pre MAE:',metrics.mean_absolute_error(y_test_true, Weighted_pre))")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\nWeighted_pre MAE: 0.05750000000000027")]),t._v(" "),r("p",[t._v("可见，单个模型的最小MAE为0.07，而简单加权后的融合模型的MAE为0.06，有了提升。还有其他一些平均加权的方式，比如各个模型对应的各个label取平均或者取中位数。")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\ndef Mean_method(test_pre1,test_pre2,test_pre3):\nMean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),\npd.Series(test_pre3)], axis=1).mean(axis=1)\nreturn Mean_result")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nMean_pre = Mean_method(test_pre1,test_pre2,test_pre3)\nprint('Mean_pre MAE:',metrics.mean_absolute_error(y_test_true, Mean_pre))")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nMean_pre MAE: 0.06666666666666693")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n## 定义结果的加权平均函数\ndef Median_method(test_pre1,test_pre2,test_pre3):\nMedian_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\nreturn Median_result")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nMedian_pre = Median_method(test_pre1,test_pre2,test_pre3)\nprint('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre))")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nMedian_pre MAE: 0.07500000000000007")]),t._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[t._v("Stacking融合（回归）")])]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\nfrom sklearn import linear_model")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("def Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,\n                   test_pre1, test_pre2, test_pre3, \n                    model_L2=linear_model.LinearRegression()):\n    model_L2.fit(pd.concat([pd.Series(train_reg1), pd.Series(train_reg2),\n                           pd.Series(train_reg3)], axis=1).values, y_train_true)\n    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1), pd.Series(test_pre2),\n                                                 pd.Series(test_pre3)],axis=1).values)\n    return Stacking_result\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值\ntrain_reg1 = [3.2, 8.2, 9.1, 5.2]\ntrain_reg2 = [2.9, 8.1, 9.0, 4.9]\ntrain_reg3 = [3.1, 7.9, 9.2, 5.0]\n# y_test_true 代表第模型的真实值\ny_train_true = [3, 8, 9, 5]")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("test_pre1 = [1.2, 3.2, 2.1, 6.2]\ntest_pre2 = [0.9, 3.1, 2.0, 5.9]\ntest_pre3 = [1.1, 2.9, 2.2, 6.0]\n\n# y_test_true 代表第模型的真实值\ny_test_true = [1, 3, 2, 6] \n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\nmodel_L2 = linear_model.LinearRegression()\nStacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,\ntest_pre1,test_pre2,test_pre3,model_L2)\nprint('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre))")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nStacking_pre MAE: 0.04213483146067476")]),t._v(" "),r("p",[t._v("此处采用线性回归模型作为次级模型，对于第二层Stacking的模型不宜选取的过于复杂，这样会导致模型在训练集上过拟合，从而使得在测试集上并不能达到很好的效果。")]),t._v(" "),r("h3",{attrs:{id:"分类模型融合"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#分类模型融合"}},[t._v("#")]),t._v(" 分类模型融合")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\nfrom sklearn.datasets import make_blobs\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold")]),t._v(" "),r("ol",[r("li",[t._v("Voting投票机制")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* Voting即投票机制，分为软投票和硬投票两种，其原理采用少数服从多数的思想。\n* 软投票：和硬投票原理相同，增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。\n* 硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n# 硬投票\niris = datasets.load_iris()")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("x = iris.data\ny = iris.target\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n\nclf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2,\n                    subsample=0.7, colsample_bytree=0.6, object='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n                             min_samples_leaf=63, oob_score=True)\nclf3 = SVC(C=0.1)\n\neclf = VotingClassifier(estimators=[('xgb',clf1),('rf',clf2),('svc',clf3)], voting='hard')\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nAccuracy: 0.96 (+/- 0.02) [XGBBoosting]\nAccuracy: 0.33 (+/- 0.00) [Random Forest]\nAccuracy: 0.95 (+/- 0.03) [SVM]\nAccuracy: 0.95 (+/- 0.03) [Ensemble]")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n# 软投票\nx=iris.data\ny=iris.target\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.8,\n                     colsample_bytree=0.8, objective='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n                              min_samples_leaf=63,oob_score=True)\nclf3 = SVC(C=0.1, probability=True)\n\n# 软投票\neclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 1])\nclf1.fit(x_train, y_train)\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nAccuracy: 0.96 (+/- 0.02) [XGBBoosting]\nAccuracy: 0.33 (+/- 0.00) [Random Forest]\nAccuracy: 0.95 (+/- 0.03) [SVM]\nAccuracy: 0.96 (+/- 0.02) [Ensemble]")]),t._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[t._v("分类的Stacking")])]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n'''\n5-Fold Stacking\n'''\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier\nimport pandas as pd\n#创建训练的数据集\niris = datasets.load_iris()\ndata_0 = iris.data\ndata = data_0[:100,:]\nprint(data.shape)\ntarget_0 = iris.target\ntarget = target_0[:100]\nprint(target.shape)")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\n(100, 4)\n(100,)")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n#模型融合中使用到的各个单模型\nclfs = [LogisticRegression(solver='lbfgs'),\nRandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\nExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\nExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\nGradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("#切分一部分数据作为测试集\nX, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n\ndataset_blend_train = np.zeros((X.shape[0], len(clfs)))\ndataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n\nprint(dataset_blend_train.shape)\nprint(dataset_blend_test.shape)\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n(70, 5)\n(30, 5)")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n#5折stacking\n# n_splits = 5\n# skf = StratifiedKFold(n_splits)\n# skf = skf.split(X, y)")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n# 依次训练各个单模型\nfor j, clf in enumerate(clfs):  # 索引和对应的模型\nprint('='*30, '\\n', j)\ndataset_blend_test_j = np.zeros((X_predict.shape[0], 5))  # shape:(30, 5)")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v('    n_splits = 5\n    skf = StratifiedKFold(n_splits)\n    skf = skf.split(X, y)\n\n    # 5-Fold交叉训练\n    for i, (train, test) in enumerate(skf):  # 总样本数100，train样本70，五折后分成56个样本训练和14个样本验证 \n        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]  # train和test对应样本的索引\n        clf.fit(X_train, y_train)\n        y_submission = clf.predict_proba(X_test)[:, 1]  # 预测的各个标签值为1的概率\n        dataset_blend_train[test, j] = y_submission  # dataset_blend_train有5列，将y_submission存储到第j列的对应test索引中\n        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n    #对于测试集，直接用这k个模型的预测值均值作为新的特征。\n    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)  # mean()中的1表示求每一行的均值\n\n    print("val auc Score: %f" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n')])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\n==============================\n0\nval auc Score: 1.000000\n==============================\n1\nval auc Score: 1.000000\n==============================\n2\nval auc Score: 1.000000\n==============================\n3\nval auc Score: 1.000000\n==============================\n4\nval auc Score: 1.000000")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nclf = LogisticRegression(solver='lbfgs')\nclf.fit(dataset_blend_train, y)\ny_submission = clf.predict_proba(dataset_blend_test)[:, 1]")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v('print("Val auc Score of Stacking: %f" % (roc_auc_score(y_predict, y_submission)))\n')])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nVal auc Score of Stacking: 1.000000")]),t._v(" "),r("ol",{attrs:{start:"3"}},[r("li",[t._v("Blending")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 直接看程序比较清楚\n* 优点： \n  * 比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）\n  * 避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集\n* 缺点： \n  * 使用了很少的数据\n  * blender可能会过拟合\n  * stacking使用多次的交叉验证会比较稳健\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n#创建训练的数据集\n#创建训练的数据集\ndata_0 = iris.data\ndata = data_0[:100,:]")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("target_0 = iris.target\ntarget = target_0[:100]\n\n#模型融合中使用到的各个单模型\nclfs = [LogisticRegression(solver='lbfgs'),\n        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n\n#切分一部分数据作为测试集\nX, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n\n#切分训练数据集为d1,d2两部分\nX_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020)\ndataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\ndataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n\nfor j, clf in enumerate(clfs):\n    #依次训练各个单模型\n    clf.fit(X_d1, y_d1)\n    y_submission = clf.predict_proba(X_d2)[:, 1]  # 验证集的预测\n    dataset_d1[:, j] = y_submission\n    #对于测试集，直接用这k个模型的预测值作为新的特征。\n    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]  # 测试集的预测\n    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n\n#融合使用的模型\nclf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\nclf.fit(dataset_d1, y_d2)  # 验证集的预测值与实际标签值\ny_submission = clf.predict_proba(dataset_d2)[:, 1]  # 用测试集的预测值预测\nprint(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\n​"),r("br"),t._v("\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nVal auc Score of Blending: 1.000000")]),t._v(" "),r("ol",{attrs:{start:"4"}},[r("li",[t._v("分类的Stacking融合")])]),t._v(" "),r("p",[t._v("利用mlxtend，直接从其中引入StackingClassifier，以及画图")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\nimport warnings\nwarnings.filterwarnings('ignore')\nimport itertools\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("from sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions\n\n# 以python自带的鸢尾花数据集为例\niris = datasets.load_iris()\nX, y = iris.data[:, 1:3], iris.target\n\nclf1 = KNeighborsClassifier(n_neighbors=1)  # KNN\nclf2 = RandomForestClassifier(random_state=1)  # 随机森林\nclf3 = GaussianNB()  # 朴素贝叶斯\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n\nlabel = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\nclf_list = [clf1, clf2, clf3, sclf]\n\nfig = plt.figure(figsize=(10,8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nclf_cv_mean = []\nclf_cv_std = []\nfor clf, label, grd in zip(clf_list, label, grid):\n\n    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n    clf_cv_mean.append(scores.mean())\n    clf_cv_std.append(scores.std())\n\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf)\n    plt.title(label)\n\nplt.show()\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nAccuracy: 0.91 (+/- 0.01) [KNN]\nAccuracy: 0.93 (+/- 0.05) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [Naive Bayes]\nAccuracy: 0.95 (+/- 0.03) [Stacking Classifier]")]),t._v(" "),r("p",[r("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200404174149659.png",alt:""}})]),t._v(" "),r("h3",{attrs:{id:"其他融合方法"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#其他融合方法"}},[t._v("#")]),t._v(" 其他融合方法")]),t._v(" "),r("p",[t._v("加入新的特征")]),t._v(" "),r("p",[t._v("​"),r("br"),t._v("\ndef Ensemble_add_feature(train, test, target, clfs):\ntrain_ = np.zeros((train.shape[0], len(clfs"),r("em",[t._v("2)))\ntest_ = np.zeros((test.shape[0], len(clfs")]),t._v("2)))")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("    for j,clf in enumerate(clfs):\n        clf.fit(train, target)\n        y_train = clf.predict(train)\n        y_test = clf.predict(test)\n\n        # 生成新特征\n        train_[:, j*2] = y_train**2\n        test_[:, j*2] = y_test**2\n        train_[:, j+1] = np.exp(y_train)\n        test_[:, j+1] = np.exp(y_test)\n        print('Method', j)\n\n    train_ = pd.DataFrame(train_)\n    test_ = pd.DataFrame(test_)\n    return train_, test_\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression")]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("clf = LogisticRegression()\ndata_0 = iris.data\ndata = data_0[:100,:]\n\ntarget_0 = iris.target\ntarget = target_0[:100]\n\nx_train,x_test,y_train,y_test=train_test_split(data,target,test_size=0.3)\nx_train = pd.DataFrame(x_train) ; x_test = pd.DataFrame(x_test)\n\n#模型融合中使用到的各个单模型\nclfs = [LogisticRegression(),\n        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n\nNew_train, New_test = Ensemble_add_feature(x_train, x_test, y_train, clfs)\n\nclf = LogisticRegression()\nclf.fit(New_train, y_train)\ny_emb = clf.predict_proba(New_test)[:,1]\n\nprint(\"Val auc Score of stacking: %f\" % (roc_auc_score(y_test, y_emb)))\n")])])]),r("p",[t._v("​"),r("br"),t._v("\n​"),r("br"),t._v("\nMethod 0\nMethod 1\nMethod 2\nMethod 3\nMethod 4\nVal auc Score of stacking: 1.000000")]),t._v(" "),r("h2",{attrs:{id:"经验总结"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#经验总结"}},[t._v("#")]),t._v(" 经验总结")]),t._v(" "),r("ol",[r("li",[t._v("结果层面的融合")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 最常见的融合方法，比如加权而哦那个和\n")])])]),r("ol",{attrs:{start:"2"}},[r("li",[t._v("模型层面的融合")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 涉及模型的堆叠和设计，比如Stacking\n")])])]),r("ol",{attrs:{start:"3"}},[r("li",[t._v("特征层面的融合")])]),t._v(" "),r("div",{staticClass:"language- extra-class"},[r("pre",[r("code",[t._v("* 用多种模型训练时，将特征切分给不同的模型\n")])])])])}),[],!1,null,null,null);e.default=s.exports}}]);