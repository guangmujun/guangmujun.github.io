(window.webpackJsonp=window.webpackJsonp||[]).push([[88],{664:function(e,n,a){"use strict";a.r(n);var r=a(7),s=Object(r.a)({},(function(){var e=this,n=e.$createElement,a=e._self._c||n;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"建立模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#建立模型"}},[e._v("#")]),e._v(" 建立模型")]),e._v(" "),a("h3",{attrs:{id:"读取数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#读取数据"}},[e._v("#")]),e._v(" 读取数据")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')")]),e._v(" "),a("p",[e._v("reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v('\ndef reduce_mem_usage(df):\n""" iterate through all the columns of a dataframe and modify the data type\nto reduce memory usage.'),a("br"),e._v('\n"""\nstart_mem = df.memory_usage().sum()\nprint(\'Memory usage of dataframe is {:.2f} MB\'.format(start_mem))')]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df\n")])])]),a("p",[e._v("​"),a("br"),e._v("\n​"),a("br"),e._v("\nsample_feature = reduce_mem_usage(pd.read_csv('data_for_tree.csv'))")]),e._v(" "),a("p",[e._v("Memory usage of dataframe is 58915080.00 MB Memory usage after optimization\nis: 16321162.00 MB Decreased by 72.3%\n查看各个特征的类型，发现notRepairedDamage特征的类型由object转换为float16")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nsample_feature.info()")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\n​"),a("br"),e._v("\ncontinuous_feature_names = [x for x in sample_feature.columns if x not in ['price','brand','model']]")]),e._v(" "),a("p",[e._v("筛选出连续变量，用于线性回归")]),e._v(" "),a("h3",{attrs:{id:"简单线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简单线性回归"}},[e._v("#")]),e._v(" 简单线性回归")]),e._v(" "),a("p",[e._v("dropna函数删除了含有nan值的记录，由下可见删除了有9万条左右，reset_index重新设置索引。")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nsample_feature = sample_feature.dropna().replace('-', 0).reset_index(drop=True)")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\n​"),a("br"),e._v("\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression(normalize=True)\nmodel = model.fit(train_X, train_y)\n'intercept:' + str(model.intercept_)\nsorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=lambda x:x[1], reverse=True)")]),e._v(" "),a("p",[e._v("sorted函数排序，第二个参数key是用来进行比较的元素，其中x取自前面的字典item，x[1]表示特征对应的系数，所以意思是按照特征系数的大小的名称降序排列。\n对标签进行log(x+1)的变换，使其贴近于正态分布，之所以+1，是为了让price全部为正值。")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\ntrain_y_ln = np.log(train_y + 1)")]),e._v(" "),a("h3",{attrs:{id:"五折交叉验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#五折交叉验证"}},[e._v("#")]),e._v(" 五折交叉验证")]),e._v(" "),a("p",[e._v("数据集一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这部分就是好评估集）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross\nValidation）")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, make_scorer")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\n​"),a("br"),e._v("\ndef log_transfer(func):\ndef wrapper(y, yhat):\nresult = func(np.log(y), np.nan_to_num(np.log(yhat)))\nreturn result\nreturn wrapper")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\n​"),a("br"),e._v("\nscores = cross_val_score(model, X=train_X, y=train_y, verbose=1, cv = 5,\nscoring=make_scorer(log_transfer(mean_absolute_error)))")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\n​"),a("br"),e._v("\nscores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=1, cv = 5, scoring=make_scorer(mean_absolute_error))")]),e._v(" "),a("h2",{attrs:{id:"模型调参"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型调参"}},[e._v("#")]),e._v(" 模型调参")]),e._v(" "),a("p",[e._v("以LGB为例 网格调参和贪心调参比较好理解，网格调参十分耗时，贪心调参容易陷入局部最优，贝叶斯调整速度快效果好。")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\n## LGB的参数集合：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']\n\nnum_leaves = [3,5,10,15,20,40, 55]\nmax_depth = [3,5,10,15,20,40, 55]\nbagging_fraction = []\nfeature_fraction = []\ndrop_rate = []\n")])])]),a("h3",{attrs:{id:"grid-search调参"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#grid-search调参"}},[e._v("#")]),e._v(" Grid Search调参")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'objective': objective , 'num_leaves': num_leaves, 'max_depth': max_depth}\nmodel = LGBMRegressor()\nclf = GridSearchCV(model, parameters, cv=5)\nclf = clf.fit(train_X, train_y)\nclf.best_params_\nmodel = LGBMRegressor(objective='regression',\nnum_leaves=55,\nmax_depth=15)\nnp.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))")]),e._v(" "),a("h3",{attrs:{id:"贪心调参"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#贪心调参"}},[e._v("#")]),e._v(" 贪心调参")]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nbest_obj = dict()\nfor obj in objective:\nmodel = LGBMRegressor(objective=obj)\nscore = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))\nbest_obj[obj] = score")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("best_leaves = dict()\nfor leaves in num_leaves:\n    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)\n    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))\n    best_leaves[leaves] = score\n\nbest_depth = dict()\nfor depth in max_depth:\n    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],\n                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],\n                          max_depth=depth)\n    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)))\n    best_depth[depth] = score\n")])])]),a("h3",{attrs:{id:"贝叶斯调参"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#贝叶斯调参"}},[e._v("#")]),e._v(" 贝叶斯调参")]),e._v(" "),a("p",[e._v("安装"),a("code",[e._v("pip install bayesian-optimization")])]),e._v(" "),a("p",[e._v("​"),a("br"),e._v("\nfrom bayes_opt import BayesianOptimization")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("def rf_cv(num_leaves, max_depth, subsample, min_child_samples):\n    val = cross_val_score(\n        LGBMRegressor(objective = 'regression_l1',\n            num_leaves=int(num_leaves),\n            max_depth=int(max_depth),\n            subsample = subsample,\n            min_child_samples = int(min_child_samples)\n        ),\n        X=train_X, y=train_y_ln, verbose=0, cv = 5, scoring=make_scorer(mean_absolute_error)\n    ).mean()\n    return 1 - val\nrf_bo = BayesianOptimization(\n    rf_cv,\n    {\n    'num_leaves': (2, 100),\n    'max_depth': (2, 100),\n    'subsample': (0.1, 1),\n    'min_child_samples' : (2, 100)\n    }\n)\nrf_bo.maximize()\n1 - rf_bo.max['target']\n")])])])])}),[],!1,null,null,null);n.default=s.exports}}]);