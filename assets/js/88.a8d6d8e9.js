(window.webpackJsonp=window.webpackJsonp||[]).push([[88],{664:function(a,t,n){"use strict";n.r(t);var r=n(7),e=Object(r.a)({},(function(){var a=this,t=a.$createElement,n=a._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[n("h2",{attrs:{id:"基本介绍"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#基本介绍"}},[a._v("#")]),a._v(" 基本介绍")]),a._v(" "),n("ol",[n("li",[a._v("重要性 调参效果有限，特征工程的好坏决定最终的排名和成绩")]),a._v(" "),n("li",[a._v("目的 将数据转换为能更好地表示潜在问题的特征")])]),a._v(" "),n("h2",{attrs:{id:"内容介绍-精华"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#内容介绍-精华"}},[a._v("#")]),a._v(" 内容介绍（精华）")]),a._v(" "),n("p",[a._v("说明：以下内容中，加粗的部分为实战中使用到的方法，有具体的实现代码，剩余的相关处理技术后续再补充上。 常见的特征工程包括： 1. 异常处理： - 通过\n"),n("strong",[a._v("箱线图")]),a._v(" （或3-Sigma）分析删除异常值 - BOX-COX转换（处理有偏分布） - 长尾截断 2. 特征归一化/标准化： -\n标准化（转换为标准正态分布） - "),n("strong",[a._v("归一化")]),a._v(" （转换到[0, 1]区间） - 针对幂律分布，可以采用公式：$\\log\n\\left(\\frac{1+x}{1+m e \\operatorname{dian}}\\right)$ 3. 数据分桶： - 等频分桶 -\n"),n("strong",[a._v("等距分桶")]),a._v(" - Best-KS分桶（类似利用基尼指数进行二分类） - 卡方分桶 4. 缺失值处理： - "),n("strong",[a._v("不处理")]),a._v("\n（针对类似XGBoost等树模型） - 删除（特征缺失的数据太多，可以考虑删除） -\n插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等 - 分箱，缺失值一个箱 5. 特征构造： - "),n("strong",[a._v("构造统计量特征")]),a._v("\n，报告计数，求和，比例，标准差等 - "),n("strong",[a._v("时间特征")]),a._v(" ，包括相对时间和绝对时间，节假日，双休日等 - "),n("strong",[a._v("地理信息")]),a._v(" ，包括分箱，分布编码等方法\n- "),n("strong",[a._v("非线性变换")]),a._v(" ，包括log/平方/根号等 - 特征组合，特征交叉 - 仁者见仁，智者见智 6. 特征筛选 -\n过滤式（filter）：先对数据进行特征选择，然后再训练学习器，常见的方法有Relief/方差选择法/相关系数法/卡方检验法/互信息法 -\n包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有LVM（Las Vegas Wrapper） -\n"),n("strong",[a._v("嵌入式")]),a._v(" （embedding）：结果过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有lasso回归 7. 降维 -\nPCA/LDA/ICA")]),a._v(" "),n("h2",{attrs:{id:"代码示例"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#代码示例"}},[a._v("#")]),a._v(" 代码示例")]),a._v(" "),n("h3",{attrs:{id:"导入数据"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#导入数据"}},[a._v("#")]),a._v(" 导入数据")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[a._v("Train_data = pd.read_csv('used_car_train_20200313.csv', sep=' ')\nTest_data = pd.read_csv('used_car_testA_20200313.csv', sep=' ')\nprint(Train_data.shape)\nprint(Test_data.shape)\n")])])]),n("h3",{attrs:{id:"删除异常值"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#删除异常值"}},[a._v("#")]),a._v(" 删除异常值")]),a._v(" "),n("p",[a._v("下面为利用 "),n("strong",[a._v("箱线图")]),a._v(" 剔除异常值的函数")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v('\ndef outliers_proc(data, col_name, scale=3):\n"""\n用于清洗异常值，默认用 box_plot（scale=3）进行清洗\n:param data: 接收 pandas 数据格式\n:param col_name: pandas 列名\n:param scale: 尺度\n:return:\n"""')]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[a._v('    def box_plot_outliers(data_ser, box_scale):\n        """\n        利用箱线图去除异常值\n        :param data_ser: 接收 pandas.Series 数据格式\n        :param box_scale: 箱线图尺度，\n        :return:\n        """\n        iqr = box_scale * (data_ser.quantile(0.75) - data_ser.quantile(0.25))\n        val_low = data_ser.quantile(0.25) - iqr\n        val_up = data_ser.quantile(0.75) + iqr\n        rule_low = (data_ser < val_low)\n        rule_up = (data_ser > val_up)\n        return (rule_low, rule_up), (val_low, val_up)\n\n    data_n = data.copy()\n    data_series = data_n[col_name]\n    rule, value = box_plot_outliers(data_series, box_scale=scale)\n    index = np.arange(data_series.shape[0])[rule[0] | rule[1]]\n    print("Delete number is: {}".format(len(index)))\n    data_n = data_n.drop(index)\n    data_n.reset_index(drop=True, inplace=True)\n    print("Now column number is: {}".format(data_n.shape[0]))\n    index_low = np.arange(data_series.shape[0])[rule[0]]\n    outliers = data_series.iloc[index_low]\n    print("Description of data less than the lower bound is:")\n    print(pd.Series(outliers).describe())\n    index_up = np.arange(data_series.shape[0])[rule[1]]\n    outliers = data_series.iloc[index_up]\n    print("Description of data larger than the upper bound is:")\n    print(pd.Series(outliers).describe())\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n    sns.boxplot(y=data[col_name], data=data, palette="Set1", ax=ax[0])\n    sns.boxplot(y=data_n[col_name], data=data_n, palette="Set1", ax=ax[1])\n    return data_n\n')])])]),n("p",[a._v("实战中，删除power特征的异常数据。 "),n("strong",[a._v("注意：测试集的数据不能删除")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nTrain_data = outliers_proc(Train_data, 'power', scale=3)")]),a._v(" "),n("p",[a._v("运行结果：")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200325211540241.png",alt:""}})]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200325211658549.png",alt:""}})]),a._v(" "),n("h3",{attrs:{id:"特征构造"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#特征构造"}},[a._v("#")]),a._v(" 特征构造")]),a._v(" "),n("ol",[n("li",[a._v("测试集和训练集放到一起，对特征进行处理。")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nTrain_data['train']=1\nTest_data['train']=0\ndata = pd.concat([Train_data, Test_data], ignore_index=True)")]),a._v(" "),n("ol",{attrs:{start:"2"}},[n("li",[n("strong",[a._v("时间特征")]),a._v(" 处理 使用时间"),n("code",[a._v("data['creatDate'] - data['regDate']")]),a._v("反应汽车使用时间，一般来说价格与使用时间成反比。不过要注意，数据里有时间出错的格式，所以我们需要使用"),n("code",[a._v("errors='coerce'")]),a._v("将无效值强制转换为NaN。")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata['used_time'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') -\npd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[a._v("data['used_time'].head()\n")])])]),n("ol",{attrs:{start:"3"}},[n("li",[n("strong",[a._v("缺失值处理")]),a._v(" ，看一下空数据，有 约15k 个样本的时间是有问题的，我们可以选择删除，也可以选择放着。但是这里不建议删除，因为删除缺失数据占总样本量过大，7.5%。我们可以先放着，因为如果我们 XGBoost 之类的决策树，其本身就能处理缺失值，所以可以不用管；")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata['used_time'].isnull().sum()")]),a._v(" "),n("ol",{attrs:{start:"4"}},[n("li",[n("strong",[a._v("地理编码")]),a._v(" 处理 从邮编中提取城市信息，相当于加入了先验知识。提取"),n("code",[a._v("regionCode")]),a._v("的第一位代表不同的城市。")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata['city'] = data['regionCode'].apply(lambda x: str(x)[:-3])\ndata = data\ndata['city'].head()")]),a._v(" "),n("ol",{attrs:{start:"5"}},[n("li",[n("strong",[a._v("构造统计量特征")]),a._v(" 此处计算某品牌的销售统计量，也可以计算其他特征的统计量，这里要用train 的数据计算统计量。")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nTrain_gb = Train_data.groupby('brand')  # 按找brand这一列分组\nall_info = {}  # 键：某一品牌，值：这一品牌车的统计信息\nfor kind, kind_data in Train_gb:  # kind表示具体的一类，kind_data表示这一类中的所有数据\ninfo = {}\nkind_data = kind_data[kind_data['price'] > 0]  # 挑选出这一类中price>0的\ninfo['brand_amount'] = len(kind_data)  # 统计出这一品牌中车的总数\ninfo['brand_price_max'] = kind_data.price.max()  # 这一品牌的车种价格最高是多少\ninfo['brand_price_median'] = kind_data.price.median()\ninfo['brand_price_min'] = kind_data.price.min()\ninfo['brand_price_sum'] = kind_data.price.sum()\ninfo['brand_price_std'] = kind_data.price.std()\ninfo['brand_price_average'] = round(kind_data.price.sum() / (len(kind_data)+1), 2)\nall_info[kind] = info")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[a._v("# 字典转变为DataFrame;还原索引，从新变为默认的整型索引;重命名类名\nbrand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={'index':'brand'})\nbrand_fe.head()\n")])])]),n("p",[a._v("运行结果示例：")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200325213350909.png",alt:""}})]),a._v(" "),n("p",[a._v("将上述分组统计的数据，合并到原来的数据集中")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata = data.merge(brand_fe, how='left', on='brand')\ndata.head()")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/2020032521352241.png",alt:""}})]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200325213547161.png",alt:""}})]),a._v(" "),n("h3",{attrs:{id:"数据分桶"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#数据分桶"}},[a._v("#")]),a._v(" 数据分桶")]),a._v(" "),n("ol",[n("li",[a._v("数据分桶的原因\n"),n("ol",[n("li",[a._v("离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；\n2. 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；\n3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；\n4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；\n5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化\n6. LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性")])])]),a._v(" "),n("li",[a._v("此处使用 "),n("strong",[a._v("等距数据分桶")]),a._v(" 以 power 为例，这时候我们的缺失值也进桶了。")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\n# bin为0-300，间距为10的等差数列,将发动机功率分成30个类别\nbin = [i*10 for i in range(31)]"),n("br"),a._v("\n# 用来把一组数据分割成离散的区间\ndata['power_bin'] = pd.cut(data['power'], bin, labels=False)"),n("br"),a._v("\n# power_bin为0-29，共30个类\ndata[['power_bin', 'power']].head()")]),a._v(" "),n("p",[a._v("最后，删除不需要的数据，axis=1表示删除一列。")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata = data.drop(['creatDate', 'regDate', 'regionCode'], axis=1)\ndata.columns")]),a._v(" "),n("p",[a._v("结果：")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200325214942641.png",alt:""}})]),a._v(" "),n("p",[a._v("目前的数据其实已经可以给树模型使用了，所以我们导出一下。")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata.to_csv('data_for_tree.csv', index=0)")]),a._v(" "),n("h3",{attrs:{id:"特征构造-lr-nn"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#特征构造-lr-nn"}},[a._v("#")]),a._v(" 特征构造（LR,NN）")]),a._v(" "),n("p",[a._v("不同模型对数据集的要求不同，所以分开构造 1. 变换分布")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nTrain_data['power'].plot.hist()")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200326092900961.png",alt:""}})]),a._v(" "),n("p",[a._v("原数据集不是正态分布，我们对其取 log，在做归一化。")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata['power'] = np.log(data['power'] + 1)  # +1为了让取log后的值大于0\ndata['power'] = ((data['power'] - np.min(data['power'])) / (np.max(data['price']) - np.min(data['price'])))\ndata['power'].plot.hist()")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200326093026942.png",alt:""}})]),a._v(" "),n("ol",{attrs:{start:"2"}},[n("li",[a._v("归一化")])]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndef max_min(x):\nreturn (x - np.min(x)) / (np.max(x) - np.min(x))")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[a._v("data['kilometer'] = max_min(data['kilometer'])\ndata['brand_amount'] = max_min(data['brand_amount'])\ndata['brand_price_average'] = max_min(data['brand_price_average'])\ndata['brand_price_max'] = max_min(data['brand_price_max'])\ndata['brand_price_median'] = max_min(data['brand_price_median'])\ndata['brand_price_min'] = max_min(data['brand_price_min'])\ndata['brand_price_std'] = max_min(data['brand_price_std'])\ndata['brand_price_sum'] = max_min(data['brand_price_sum'])\n")])])]),n("ol",{attrs:{start:"3"}},[n("li",[a._v("类别特征编码")])]),a._v(" "),n("p",[a._v("名义变量转换成哑元变量，利用pandas实现one hot\nencode，可参考"),n("a",{attrs:{href:"https://blog.csdn.net/maymay_/article/details/80198468",target:"_blank",rel:"noopener noreferrer"}},[a._v("网址"),n("OutboundLink")],1),a._v("。")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata = pd.get_dummies(data, columns=['model', 'brand', 'bodyType', 'fuelType', 'gearbox',\n'notRepairedDamage', 'power_bin'])\ndata.columns")]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/202003260939101.png",alt:""}})]),a._v(" "),n("p",[a._v("最后，存储数据给LR用")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata.to_csv('data_for_lr.csv', index=0)")]),a._v(" "),n("h3",{attrs:{id:"特征筛选"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#特征筛选"}},[a._v("#")]),a._v(" 特征筛选")]),a._v(" "),n("ol",[n("li",[a._v("过滤式")])]),a._v(" "),n("p",[a._v("相关性分析，从相关性较大的特征之间，去除一个，可以计算出相关系数，或者看相关性矩阵图。其中，"),n("code",[a._v("method")]),a._v("参数的值可以是： -\npearson：来衡量两个数据集合是否在一条线上面，即针对线性数据的相关系数计算，针对非线性数据便会有误差。 -\nkendall：用于反映分类变量相关性的指标，即针对无序序列的相关系数，非正太分布的数据 - spearman：非线性的，非正太分析的数据的相关系数")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nprint(data['power'].corr(data['price'], method='spearman'))\nprint(data['kilometer'].corr(data['price'], method='spearman'))")]),a._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[a._v("data_numeric = data[['power', 'kilometer', 'brand_amount', 'brand_price_average',\n                    'brand_price_max', 'brand_price_median']]\ncorrelation = data_numeric.corr()\nf, ax = plt.subplots(figsize = (7, 7))\nplt.title('Correlation of Numeric Features with Price', y=1, size=16)\nsns.heatmap(correlation, square=True, vmax=0.8)\n")])])]),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200326094441960.png",alt:""}})]),a._v(" "),n("ol",{attrs:{start:"2"}},[n("li",[a._v("包裹式")])]),a._v(" "),n("p",[a._v("没有详细研究，单纯记录下，使用"),n("code",[a._v("pip install mlxtend")]),a._v("安装。")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\nsfs = SFS(LinearRegression(),\nk_features=10,\nforward=True,\nfloating=False,\nscoring = 'r2',\ncv = 0)\nx = data.drop(['price'], axis=1)\nx = x.fillna(0)\ny = data['price']\nsfs.fit(x, y)\nsfs.k_feature_names_")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\n​"),n("br"),a._v("\n# 画出来，可以看到边际效益\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\nplt.grid()\nplt.show()")]),a._v(" "),n("ol",{attrs:{start:"3"}},[n("li",[a._v("嵌入式 "),n("strong",[a._v("大部分情况下都是使用这种方式做特征筛选！")]),a._v(" 下一章节补上")])]),a._v(" "),n("h2",{attrs:{id:"经验总结"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#经验总结"}},[a._v("#")]),a._v(" 经验总结")]),a._v(" "),n("ol",[n("li",[a._v("匿名特征的处理 （屯着……） 有些比赛的特征是匿名特征，这导致我们并不清楚特征相互直接的关联性，这时我们就只有单纯基于特征进行处理，比如装箱，groupby，agg 等这样一些操作进行一些特征统计，此外还可以对特征进行进一步的 log，exp 等变换，或者对多个特征进行四则运算（如上面我们算出的使用时长），多项式组合等然后进行筛选。由于特性的匿名性其实限制了很多对于特征的处理，当然有些时候用 NN 去提取一些特征也会达到意想不到的良好效果。")]),a._v(" "),n("li",[a._v("非匿名特征 深入分析背后的业务逻辑或者说物理原理，从而才能更好的找到 magic。")]),a._v(" "),n("li",[a._v("特征工程和模型结合 当然特征工程其实是和模型结合在一起的，这就是为什么要为 LR NN 做分桶和特征归一化的原因，而对于特征的处理效果和特征重要性等往往要通过模型来验证。")])]),a._v(" "),n("p",[a._v("机器学习"),n("a",{attrs:{href:"https://www.zhihu.com/people/is-aze/posts",target:"_blank",rel:"noopener noreferrer"}},[a._v("基础知识"),n("OutboundLink")],1)]),a._v(" "),n("h2",{attrs:{id:"问题记录"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#问题记录"}},[a._v("#")]),a._v(" 问题记录")]),a._v(" "),n("p",[a._v("Q1: 特征构造中为什么要把训练集和测试集放在一起？ A1：放到一起处理其中的特征，比如，时间转换，地理编码等特征构造措施。\n"),n("strong",[a._v("以及后面涉及到的统计量特征")]),a._v(" 。 Q2：怎么理解先验知识？\nA2：当你说：这是一幢别墅。你脑子里面是有“别墅”这个概念的，以及关于别墅的一些属性，然后你才知道你眼前这个东西叫做“别墅”。前面的“别墅”这个概念就是你对眼前建筑的先验知识。\n"),n("strong",[a._v("Q3：地理编码处理中，提取的部分样本的城市代码为空？【待处理】")]),a._v("\nA3：有的二手车的regionCode为3位数，提取出的值为空，为空的样本也可以看作是同一个城市的，但是这里并没有处理，当作缺失值的吗？？？\n"),n("strong",[a._v("Q4：构造统计量特征中，test的数据是否也需要计算统计量？（很重要）")]),a._v("\nA4：猜想：需要，训练好模型后，test数据需要有相应的统计特征，才能进行预测。 猜想不完全对。 1.\n先看下train数据的统计特征构造完后，数据融合的代码。需要注意的是，此处是将"),n("code",[a._v("brand_fe")]),a._v("的信息加入到"),n("code",[a._v("data")]),a._v("中，而此时的"),n("code",[a._v("data")]),a._v("包含test和train数据。（其中"),n("code",[a._v("brand_fe")]),a._v("是train数据"),n("code",[a._v("brand")]),a._v("特征的所有统计量）")]),a._v(" "),n("p",[a._v("​"),n("br"),a._v("\ndata = data.merge(brand_fe, how='left', on='brand')\ndata[['train', 'brand', 'brand_amount']]")]),a._v(" "),n("ol",{attrs:{start:"2"}},[n("li",[a._v("所以便有了如下的结果，test和train数据中相同"),n("code",[a._v("brand")]),a._v("的二手车，有着相同的统计特征，比如"),n("code",[a._v("brand_amount、brand_min")]),a._v("等。但是为什么test数据的"),n("code",[a._v("brand")]),a._v("统计特征的值，要用train数据统计出的值来填充？")])]),a._v(" "),n("p",[n("img",{attrs:{src:"https://img-blog.csdnimg.cn/20200327172913183.png",alt:""}})]),a._v(" "),n("ol",{attrs:{start:"3"}},[n("li",[a._v("为了保证test和train数据的样本分布情况相同，更好的适应我们用train数据训练出的模型。如果我们对test数据的"),n("code",[a._v("brand")]),a._v("特征再进行一次统计，那统计出的特征值岂不是会随着test数据的样本情况或者样本数量的改变而改变。那为什么不对"),n("code",[a._v("data")]),a._v("数据（即test和train数据）中的"),n("code",[a._v("brand")]),a._v("特征进行统计？")]),a._v(" "),n("li",[a._v("那样的话，我们就需要用"),n("code",[a._v("data")]),a._v("数据来训练模型，然后再使用额外的测试数据集来测试模型。如果尝试将"),n("code",[a._v("data")]),a._v("数据再次切分成test和train数据，以此进行模型训练和预测的话，应该也可以，test和train数据的"),n("code",[a._v("brand")]),a._v("相关统计量相同。但是此时用于train的数据的统计特征与其自身实际特征不一致，效果可能不好。")])]),a._v(" "),n("p",[a._v("Q5：数据分桶的原因没有看的太懂？ A5：")]),a._v(" "),n("ul",[n("li",[a._v("[x] 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展")]),a._v(" "),n("li",[a._v("[x] 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰")]),a._v(" "),n("li",[a._v("[x] 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化")]),a._v(" "),n("li",[a._v("[x] 离散化后可以引入特征交叉，更好的引入非线性。如果年龄分了M个桶，收入分了N个桶，则可以组合出M*N个特征。")]),a._v(" "),n("li",[a._v("[x] 分桶之后相当于引入了一个分段函数，利用这个分段函数表达了一些非线性的意义。这也就是常说的离散化之后可以增加非线性的含义。 参考："),n("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/66952177",target:"_blank",rel:"noopener noreferrer"}},[a._v("连续特征离散化的意义"),n("OutboundLink")],1)])]),a._v(" "),n("p",[a._v("Q6：LR，NN，LightGBM ，XGBoost 是啥？ A6： - LR：LinearRegression 线性回归 - NN：近邻\n(Nearest Neighbor) -\nXGBoost：一种改进后的树模型，详细参考"),n("a",{attrs:{href:"https://blog.csdn.net/a1b2c3d4123456/article/details/52849091",target:"_blank",rel:"noopener noreferrer"}},[a._v("网站"),n("OutboundLink")],1),a._v("\n-\nLightGBM：微软推出了一个新的boosting框架，想要挑战xgboost的江湖地位，详细参考"),n("a",{attrs:{href:"https://bacterous.github.io/2018/09/13/LightGBM%E4%BD%BF%E7%94%A8/",target:"_blank",rel:"noopener noreferrer"}},[a._v("网站"),n("OutboundLink")],1),a._v(" "),n("strong",[a._v("Q7：树模型和LR NN对数据集的要求是什么？")]),a._v(" A7：")]),a._v(" "),n("ol",[n("li",[a._v("xgb模型：无需提前处理缺失值，在迭代的过程中填补缺失值")]),a._v(" "),n("li",[a._v("LR模型：对于线性回归模型，对y要求正态分布，类别变量要转换为哑变量")])]),a._v(" "),n("p",[a._v("Q8：km 的比较正常，应该是已经做过分桶了，怎么看出来的？ A8：横轴为0-14的整数，应该是将汽车行驶里程数，分成了15个桶。\n"),n("strong",[a._v("Q9：长尾分布【待处理】")]),a._v(" A9：")])])}),[],!1,null,null,null);t.default=e.exports}}]);